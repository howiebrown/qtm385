{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"QTM 385 - Experimental Methods\"\n",
        "subtitle: Assignment 03\n",
        "---\n",
        "\n",
        "# Instructions\n",
        "\n",
        "This assignment covers the last two lectures of the course. As usual, it consists of 10 questions, each worth one point. You can answer the questions in any format you prefer, but I recommend using Jupyter Notebooks and converting the answers to PDF or html, as they are easier to read on Canvas. Please write at least one or two paragraphs for each written question.\n",
        "\n",
        "If you have any questions about the assignment, feel free to email me at <danilo.freire@emory.edu>.\n",
        "\n",
        "Good luck!\n",
        "\n",
        "# Questions \n",
        "\n",
        "1. Compare and contrast Type I and Type II errors. In causal inference experiments, why might a researcher be more concerned with one type of error over the other?\n",
        "\n",
        "Type I error (false positive) occurs when a researcher incorrectly rejects a true null hypothesis, concluding that an effect exists when it does not. Type II error (false negative) happens when a researcher fails to reject a false null hypothesis, missing a real effect. In simpler terms, Type I error leads to a false claim of causation, while Type II error results in overlooking a genuine causal relationship.\n",
        "\n",
        "In causal inference experiments, the concern over Type I or Type II error depends on the study's context. A researcher studying the effectiveness of a new medical treatment may prioritize minimizing Type I errors to avoid approving an ineffective or harmful drug. Conversely, in social policy experiments, a researcher might be more concerned with Type II errors, as failing to detect a real effect could mean missing an opportunity to implement beneficial policies. Balancing these errors requires careful consideration of statistical power, sample size, and significance thresholds.\n",
        "\n",
        "\n",
        "2. Explain the concept of randomisation inference and outline its advantages over traditional parametric tests, especially in the context of testing the sharp null hypothesis.\n",
        "\n",
        "Randomization inference is a nonparametric statistical approach that assesses treatment effects by comparing observed outcomes to a distribution generated under the null hypothesis through repeated random shuffling of treatment assignments. This method relies on the random assignment of treatments rather than assumptions about the underlying data distribution, making it particularly useful for small samples and non-normal data. It is often used to test the sharp null hypothesis, which states that the treatment has no effect for every unit in the study.\n",
        "\n",
        "Compared to traditional parametric tests, randomization inference has several advantages. It does not require assumptions about normality or homoskedasticity, making it more robust to violations of these conditions. Additionally, it provides exact p-values rather than relying on asymptotic approximations, increasing reliability in small samples. In causal inference, this method ensures that any detected effect is due to the treatment assignment process rather than spurious correlations, making it a powerful tool for experimental analysis.\n",
        "\n",
        "3. Compare Neyman’s hypothesis testing framework with Fisher’s sharp null hypothesis approach. What are the main advantages and disadvantages of each method in experimental settings?\n",
        "\n",
        "Neyman’s hypothesis testing framework focuses on comparing two hypotheses: the null hypothesis (\\(H_0\\)) and an alternative hypothesis (\\(H_A\\)), using a pre-specified significance level to control Type I error rates. It emphasizes repeated sampling properties, statistical power, and confidence intervals, making it well-suited for decision-making in experimental settings. However, this approach requires assumptions about population distributions and typically estimates average treatment effects (ATE), rather than individual-level effects.\n",
        "\n",
        "Fisher’s sharp null hypothesis approach, in contrast, tests whether the treatment has absolutely no effect on any individual unit in an experiment. It relies on randomization inference, generating a distribution of possible outcomes under the null by reshuffling treatment assignments. This method provides exact p-values without parametric assumptions but does not easily extend to estimating confidence intervals or alternative hypotheses. While Fisher’s method is more robust in small samples, Neyman’s framework is generally more useful for making broader inferences about population-level effects.\n",
        "\n",
        "4. Critically evaluate the use of p-values in hypothesis testing. What alternatives are suggested (or implied) in the lectures, and what are the potential benefits of these alternatives?\n",
        "\n",
        "P-values are widely used in hypothesis testing to measure the strength of evidence against a null hypothesis. However, they are often misinterpreted—many researchers mistakenly believe a p-value represents the probability that the null hypothesis is true. Additionally, p-values are sensitive to sample size, meaning that with large enough data, even trivial effects can appear statistically significant. This reliance on an arbitrary significance threshold (e.g., 0.05) also encourages \"p-hacking,\" where researchers manipulate analyses to achieve significant results, undermining the reliability of findings.\n",
        "\n",
        "Alternatives to p-values include Bayesian inference, which incorporates prior knowledge and provides a probability estimate for hypotheses, and confidence intervals, which offer a range of plausible values for an effect size rather than a binary decision. Additionally, randomization inference avoids distributional assumptions and provides exact significance tests. These alternatives emphasize effect sizes, uncertainty, and robustness over dichotomous accept/reject conclusions, leading to more nuanced and informative statistical inferences.\n",
        "\n",
        "5. The code below simulates a dataset. Modify the code so that it adds a new variable called `treat` with 500 treated individuals and 500 control individuals (complete random assignment). Also include a binary covariate called `gender` (0 = male, 1 = female; with equal probability) and update the outcome (`interviews`) by adding 2 points if the individual is female.\n",
        "\n",
        "```r\n",
        "## Set seed for reproducibility\n",
        "set.seed(385)\n",
        "\n",
        "# Load packages\n",
        "# install.packages(\"fabricatr\")\n",
        "# install.packages(\"randomizr\") # if you haven't installed them yet\n",
        "library(fabricatr)\n",
        "library(randomizr)\n",
        "\n",
        "## Simulate data\n",
        "data <- fabricate(\n",
        "  N = 1000,\n",
        "  treat = complete_ra(N = 1000, num_treated = 500),  # Assign 500 treated, 500 control\n",
        "  gender = rbinom(1000, 1, 0.5),  # Binary gender variable (0 = male, 1 = female)\n",
        "  interviews = round(rnorm(1000, mean = 10, sd = 2) + 5 * treat + 2 * gender, digits = 0)  # Adjust interviews\n",
        ")\n",
        "\n",
        "head(data)\n",
        "```\n",
        "\n",
        "6. Using the dataset created in the previous question, estimate the average treatment effect on the outcome `interviews` using the `lm_robust()` function from the `estimatr` package. Interpret the results.\n",
        "\n",
        "```r\n",
        "# Load necessary package\n",
        "# install.packages(\"estimatr\") # Uncomment if not installed\n",
        "library(estimatr)\n",
        "\n",
        "# Estimate the treatment effect using lm_robust\n",
        "ate_model <- lm_robust(interviews ~ treat, data = data)\n",
        "\n",
        "# Display results\n",
        "summary(ate_model)\n",
        "```\n",
        "\n",
        "7. Using the same dataset, estimate the average treatment effect of the treatment on the outcome `interviews` using randomisation inference. Interpret the results.\n",
        "\n",
        "```r\n",
        "# Load necessary package\n",
        "# install.packages(\"ri2\") # Uncomment if not installed\n",
        "library(ri2)\n",
        "\n",
        "# Define the sharp null hypothesis (no effect of treatment)\n",
        "sharp_null <- 0\n",
        "\n",
        "# Run randomization inference\n",
        "ri_results <- conduct_ri(\n",
        "  formula = interviews ~ treat,\n",
        "  declaration = declare_ra(N = 1000, num_arms = 2, prob_each = c(0.5, 0.5)),  # Complete randomization setup\n",
        "  assignment = data$treat,\n",
        "  outcome = data$interviews,\n",
        "  sharp_hypothesis = sharp_null\n",
        ")\n",
        "\n",
        "# Display results\n",
        "summary(ri_results)\n",
        "```\n",
        "\n",
        "8. Explain how including covariates in an experimental regression model can increase the precision of the treatment effect estimate. Under what conditions might this adjustment lead to biased estimates?\n",
        "\n",
        "Including covariates in an experimental regression model increases the precision of the treatment effect estimate by reducing unexplained variance in the outcome variable. By controlling for variables that influence the outcome but are unrelated to treatment assignment, the model can isolate the treatment effect more effectively, leading to smaller standard errors and narrower confidence intervals. This is particularly useful in cases where there is residual heterogeneity in outcomes that can be accounted for by observed covariates, improving statistical power.\n",
        "\n",
        "However, adjusting for covariates can lead to biased estimates if the covariates are post-treatment variables or if they introduce collider bias by conditioning on a variable influenced by both treatment and outcome. Additionally, if treatment assignment is not truly random (e.g., due to imperfect randomization or non-compliance), adjusting for covariates correlated with treatment can introduce endogeneity, leading to incorrect causal inferences. Therefore, while covariate adjustment enhances precision, it must be done cautiously to avoid introducing bias.\n",
        "\n",
        "9. Simulate a dataset with heterogeneous treatment effects (e.g., the treatment effect is larger for individuals with higher education). Estimate the treatment effect for different subgroups using an interaction term.\n",
        "\n",
        "```r\n",
        "# Set seed for reproducibility\n",
        "set.seed(385)\n",
        "\n",
        "# Load necessary packages\n",
        "# install.packages(\"fabricatr\")\n",
        "# install.packages(\"estimatr\")\n",
        "library(fabricatr)\n",
        "library(estimatr)\n",
        "\n",
        "# Simulate data\n",
        "data <- fabricate(\n",
        "  N = 1000,\n",
        "  treat = complete_ra(N = 1000, num_treated = 500),  # Randomly assign 500 to treatment\n",
        "  education = rbinom(1000, 1, 0.5),  # Binary education (0 = low, 1 = high)\n",
        "  interviews = round(rnorm(1000, mean = 10, sd = 2) + \n",
        "                     4 * treat + 3 * treat * education, digits = 0)  # Heterogeneous treatment effect\n",
        ")\n",
        "\n",
        "# Estimate treatment effects using an interaction term\n",
        "model <- lm_robust(interviews ~ treat * education, data = data)\n",
        "\n",
        "# Display results\n",
        "summary(model)\n",
        "```\n",
        "\n",
        "10. Why is the publication of null results important in experimental research? What are the main challenges in publishing null results, and how can the scientific community address these challenges?\n",
        "\n",
        "Publishing null results is crucial in experimental research because it helps prevent publication bias and ensures a more complete and accurate understanding of the phenomenon being studied. When only studies with statistically significant findings are published, the scientific literature becomes skewed, leading to false-positive results appearing more common than they actually are. Null results also help refine theories, guide future research, and prevent wasteful duplication of studies by informing researchers that certain interventions or treatments may not be effective.\n",
        "\n",
        "However, several challenges hinder the publication of null results. Journals often have a bias toward significant findings, making it harder for researchers to publish studies that fail to reject the null hypothesis. Additionally, researchers themselves may engage in file-drawer effects, where null results remain unpublished because they are perceived as uninteresting or career-damaging. To address these challenges, the scientific community can encourage pre-registration of studies, promote results-blind peer review, and establish journals dedicated to publishing null findings. These steps would help create a more transparent and reliable body of scientific knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### I was having trouble running the R code in VS, so below is the python code, additionally, I attached an R file with the correct R code and outputs as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:             interviews   R-squared:                       0.581\n",
            "Model:                            OLS   Adj. R-squared:                  0.580\n",
            "Method:                 Least Squares   F-statistic:                     1382.\n",
            "Date:                Wed, 12 Feb 2025   Prob (F-statistic):          1.36e-190\n",
            "Time:                        20:06:59   Log-Likelihood:                -2194.0\n",
            "No. Observations:                1000   AIC:                             4392.\n",
            "Df Residuals:                     998   BIC:                             4402.\n",
            "Df Model:                           1                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept     10.7495      0.098    110.067      0.000      10.558      10.941\n",
            "treat          5.1099      0.137     37.181      0.000       4.840       5.380\n",
            "==============================================================================\n",
            "Omnibus:                        1.377   Durbin-Watson:                   2.026\n",
            "Prob(Omnibus):                  0.502   Jarque-Bera (JB):                1.340\n",
            "Skew:                           0.090   Prob(JB):                        0.512\n",
            "Kurtosis:                       3.006   Cond. No.                         2.63\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(385)\n",
        "\n",
        "# Simulate dataset with treatment and gender\n",
        "N = 1000\n",
        "treat = np.random.choice([0, 1], size=N, p=[0.5, 0.5])  # Random assignment\n",
        "gender = np.random.choice([0, 1], size=N, p=[0.5, 0.5])  # Binary gender (0 = male, 1 = female)\n",
        "\n",
        "# Generate outcome variable 'interviews'\n",
        "interviews = np.round(np.random.normal(loc=10, scale=2, size=N) + 5 * treat + 2 * gender, decimals=0)\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({'treat': treat, 'gender': gender, 'interviews': interviews})\n",
        "\n",
        "# Estimate Average Treatment Effect (ATE) using OLS regression\n",
        "model_ate = smf.ols(\"interviews ~ treat\", data=data).fit()\n",
        "print(model_ate.summary())\n",
        "\n",
        "# Simulate dataset with heterogeneous treatment effects (education)\n",
        "education = np.random.choice([0, 1], size=N, p=[0.5, 0.5])  # Binary education (0 = low, 1 = high)\n",
        "interviews_het = np.round(np.random.normal(loc=10, scale=2, size=N) + 4 * treat + 3 * treat * education, decimals=0)\n",
        "\n",
        "# Update DataFrame with new education variable and adjusted interviews\n",
        "data['education'] = education\n",
        "data['interviews_het'] = interviews_het\n",
        "\n",
        "# Estimate treatment effects using an interaction term (treat * education)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
