---
title: QTM 385 - Experimental Methods
subtitle: Lecture 04 - Selection Bias, Variability, and Regression Analysis
author:
  - name: Danilo Freire
    email: danilo.freire@emory.edu
    affiliations: Emory University
format:
  clean-revealjs:
    self-contained: true
    code-overflow: wrap
    footer: "[Selection Bias](https://raw.githack.com/danilofreire/qtm385/main/lectures/lecture-04/04-selection-bias.html)"
transition: slide
transition-speed: default
scrollable: true
engine: knitr
revealjs-plugins:
  - multimodal
editor:
  render-on-save: true
---

# Hello, everyone! 👋 <br> Nice to see you all again! 😉 {background-color="#2d4563"}

# Brief recap 📚 {background-color="#2d4563"}

## Last time, we saw that...

:::{style="margin-top: 30px; font-size: 21px;"}
:::{.columns}
:::{.column width=50%}
- [Potential outcomes framework]{.alert} help understand treatment effects
- [Fundamental problem of causal inference]{.alert}: we only see one potential outcome
- [Treatment effect]{.alert} is the difference between potential outcomes ($\tau_{i} = Y_{1,i} - Y_{0,i}$)
- [Average causal effect]{.alert} is the goal, but simple means comparison is usually biased
- [Selection bias]{.alert}: treated/untreated groups may differ even before treatment
- [Bias]{.alert} does not vanish in large samples
- [Mathematical expectation]{.alert} $E[Y_i]$ is the population average
- [Law of large numbers]{.alert}: sample average converges to population average
- [Conditional expectation]{.alert} $E[Y_i|X_i]$ is average outcome given $X_i$
:::

:::{.column width=50%}
- [SUTVA]{.alert} (Stable Unit Treatment Value Assumption) 
- [Violations of SUTVA]{.alert}: spillovers, contamination
- [External validity]{.alert}: generalising results to other settings
- [Heterogeneous treatment effects]{.alert}: effects vary across people
- [Compliance problem]{.alert}: people may not follow random assignment
- [Intent-to-Treat (ITT)]{.alert} and [Treatment-on-the-Treated (TOT)]{.alert} analyses address compliance
:::
:::
:::

## Today, we will discuss...

:::{style="margin-top: 50px; font-size: 28px;"}
:::{.columns}
:::{.column width=50%}
- A little more about potential outcomes
- Measuring variability
- Different types of selection bias, e.g.:
  - Inappropriate controls
  - Loss to follow-up
  - Attrition bias
  - Survivorship bias
  - Post-treatment bias
- How to use `R` and `DeclareDesign` to estimate regression models
- [But first...]{.alert}
:::

:::{.column width=50%}
:::{style="text-align: center; margin-top: -40px;"}
![](figures/selection-bias.png){width=80%}

Source: [xkcd](https://xkcd.com/2618/) (where else? 😄)
:::
:::
:::
:::

## Funny correlation of the day! 

:::{style="margin-top: 30px; font-size: 21px; text-align: center;"}
![](figures/correlation.png){width=80%}
:::

# Let's get started! 🚀 {background-color="#2d4563"}

## Potential outcomes revisited

:::{style="margin-top: 30px; font-size: 25px;"}
- Remember from last time: we have two potential outcomes for each unit
- [We only observe one of them]{.alert}: $Y_i = Y_{1,i} \cdot D_i + Y_{0,i} \cdot (1 - D_i)$
  - $D_i$ is the treatment indicator = 1 if treated, 0 if not
- So we need to estimate the [average treatment effect (ATE)]{.alert}:
  - $E[Y_{1,i} - Y_{0,i}] = E[Y_{1,i}] - E[Y_{0,i}]$
- The problem is, [not all comparisons are valid]{.alert}
- I mean, they can be, but only under the [heroic assumption]{.alert} that the groups are similar on average before any adjustment
- Otherwise, we will have [selection bias]{.alert}
- Let's see how this works in practice! 🤓
:::

## Khuzdar and Maria

:::{style="margin-top: 30px; font-size: 21px;"}
:::{.columns}
:::{.column width=50%}
- [Selection bias]{.alert} occurs when the groups are systematically difference, often before the treatment (but not exclusively!)
- Using an example from Angrist and Pischke (2021), let's say we have a student called [Khuzdar]{.alert} from Kazakhstan, who is considering studying in the US and is worried about the cold weather
- Should he get health insurance? 🤔
- Let's imagine that, without insurance, Khuzdar has a potential outcome of $Y_{0,i} = 3$ and, with insurance, $Y_{1,i} = 4$. So the treatment effect is $\tau_i = 1$, that is, he gains 1 "health point" by getting insurance
:::

:::{.column width=50%}
- Now, let's imagine that Khuzdar has a Chilean colleague called [Maria Moreno]{.alert}, who is also considering studying in the US
- But since she comes from chilly Santiago, she is not worried about the cold weather
- So, without insurance, Maria has a potential outcome of $Y_{0,i} = 5$ and, with insurance, $Y_{1,i} = 5$. So the treatment effect is $\tau_i = 0$, that is, she gains no "health points" by getting insurance

:::{style="text-align: center;"}
![](figures/khuzdar-maria.png){width=80%}
:::
:::
:::
:::

## Khuzdar and Maria

:::{style="margin-top: 30px; font-size: 21px;"}
- In fact, the comparison between frail Khuzdar and hearty Maria tells us little about the causal effects of their choices!
- Why is that? Because [they were different to begin with]{.alert}
- Imagine that Khuzdar got insurance and Maria did not
- Let's do a little mathematical trick here: we will add and subtract $Y_{0, Khuzdar}$ from the treatment effect (they cancel each other out, right?)
- So we have the following:

:::{style="text-align: center;"}
![](figures/selection-bias-math.png){width=80%}
:::
:::

## Difference in means = average treatment effect + selection bias

:::{style="margin-top: 30px; font-size: 22px;"}
- So far, so good? 🤓
- If we assume that [the treatment has a constant effect]{.alert} (i.e. the treatment effect is the same for everyone), we can rewrite the equation as:

:::{style="text-align: center;"}
![](figures/selection-bias3.png){width=17%}
:::

- Where $k$ is both the individual and average causal effect of insurance on health
- Using the constant-effects model to substitute for $Avg_n[Y_{1i}|D_i = 1]$, we have

:::{style="text-align: center;"}
![](figures/selection-bias4.png){width=50%}
:::
:::

## How to check for selection bias?
### Balance tests

:::{style="margin-top: 30px; font-size: 18px;"}
:::{.columns}
:::{.column width=40%}
- We use [balance tests]{.alert} or ([randomisation checks]{.alert}) to check if the groups are similar before treatment
- The idea is to compare the means of the covariates for the treated and untreated groups
- If the means are similar, it provides evidence that nothing systematic is driving the treatment effect
- We are never 100% sure, but [we trust the random assignment]{.alert}
- Small differences in means are acceptable, as long as they are not systematic
- Why? Because some variation can happen only by chance
- [100% personal opinion]{.alert}: I think they aren't very useful 😂
- [Mutz et al (2018)](https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1322143) make a good case for that
:::

:::{.column width=60%}
:::{style="text-align: center;"}
![](figures/balance-test.png){width=80%}

Angrist and Pischke (2021), pp. 20 (selected parts)
:::
:::
:::
:::

# Questions? 🤔 {background-color="#2d4563"}

# Different types of selection bias 🎯 {background-color="#2d4563"}

## Causal diagrams and associations

:::{style="margin-top: 30px; font-size: 23px;"}
:::{.columns}
:::{.column width=50%}
- [Causal diagrams]{.alert} are a great way to visualise the relationships between variables
  - They are also called [directed acyclic graphs (DAGs)]{.alert}
  - They are "directed" because they show the direction of the relationships, and "acyclic" because they do not have loops (one cause cannot cause itself)
- They have been introduced by [Judea Pearl](https://en.wikipedia.org/wiki/Judea_Pearl) and are widely used in [causal inference]{.alert} (quick guide [here](https://www.sciencedirect.com/science/article/pii/S0895435621002407))
- They help us understand the [associations]{.alert} between variables and identify potential sources of bias
- And they are super easy to draw! 🤓
- For instance, let's illustrate a classic example of selection bias with a causal diagram (borrowed from [Hernán et al., 2019](https://doi.org/10.1097/01.ede.0000135174.63482.43))
:::

:::{.column width=50%}
:::{style="text-align: center;"}
![](figures/causal-diagram.png){width=80%}
:::

:::{style="text-align: left;"}
- Figure 1 shows the common cause $L$ (smoking) results in $E$ (carrying matches) and $D$ (lung cancer) being associated
- Figure 2 shows how randomisation breaks the association between $L$ and $D$, so the treatment effect is unbiased
- In this case, if you know what $L$ is, you can also [control for it]{.alert} in your analysis
:::
:::
:::
:::

## Some sources of selection bias
### Losses to follow-up

:::{style="margin-top: 30px; font-size: 18px;"}
:::{.columns}
:::{.column width=50%}
- One problem that we sometimes have with experiments is individuals [dropping out]{.alert}
- That is, when individuals leave the study before the end of follow-up
- Why is that an issue?
- Because the [treatment effect may be different for those who leave]{.alert}
- Imagine a follow-up study of the effect of antiretroviral therapy ($E$) on AIDS ($D$) risk among HIV-infected patients 
- $L$ is the presence of other symptoms of HIV infection 
- The greater the true level of immunosuppression ($U$), the greater the risk of AIDS. $U$ is unmeasured
- If a patient drops out from the study, his AIDS status cannot be assessed and we say that he is censored ($C=1$) 
- Patients with greater values of $U$ are more likely to be lost to follow up
- The [causal diagram]{.alert} shows the relationship between the variables
:::

:::{.column width=50%}
:::{style="text-align: center;"}
![](figures/selective-survival.png){width=80%}
:::

:::{style="text-align: left;"}
- How can we correct for this bias?
- One way is to use [stratified analysis]{.alert} or [inverse probability weighting]{.alert}
- We will discuss these methods in further detail in the next lectures
- But stratification is quite simple:
  - Assuming that $L$ is measured, we can estimate the treatment effect within each stratum of $L$
  - Then we can average the stratum-specific estimates to get the overall treatment effect
:::
:::
:::
:::

## Attrition bias
### Closely related to losses to follow-up

:::{style="margin-top: 30px; font-size: 24px;"}
- [Attrition bias]{.alert} is a type of selection bias that occurs when individuals drop out of a study in a way that is related to the treatment or outcome
- It is [more systematic]{.alert} than losses to follow-up, which can happen by chance (e.g., disengagement, relocation, or other factors, leading to incomplete data)
- For example, in a clinical trial, if participants experiencing side effects disproportionately drop out of the treatment group compared to the placebo group, this difference introduces attrition bias
- In short, attrition bias specifically addresses the bias that arises when these [departures are non-random]{.alert}
- As with losses to follow-up, this is a type of selection bias [than can be caused by the experiment itself]{.alert}, not only by pre-existing differences between groups
:::

## Survivorship bias

:::{style="margin-top: 30px; font-size: 24px;"}
- In my view, one of the coolest examples of selection bias
- [Survivorship bias]{.alert} is the logical error of concentrating on the people or things that made it past some selection process and overlooking those that did not
- It is also [closely related to attrition bias and follow up bias]{.alertt} (more mentioned in observational data)
  - Attrition bias emphasises [the process of losing data]{.alert}, while survivorship bias highlights [the exclusion of failures in analysis]{.alert}
- For instance, imagine that you are studying the [effect of education on income]{.alert}
- If you only look at people who have completed college, you are ignoring those who dropped out
- This can lead to an [overestimation]{.alert} of the effect of education on income
- Why? Because those who dropped out may have done so because they were not good students, which can affect their income
- Have a look at this example (you may have seen it before!)
:::
## Survivorship bias in World War II aircraft
:::{style="margin-top: 30px; font-size: 23px;"}
:::{.columns}
:::{.column width=50%}
- During World War II, the US military wanted to [improve the armour on its aircraft]{.alert}
- They analysed the [bullet holes]{.alert} on returning aircraft and decided to add more armour to the areas with the most bullet holes
- Considering the picture you see here, where would you add the armour?
- More [here](https://en.wikipedia.org/wiki/Survivorship_bias#Military)

:::{style="text-align: margin-top: 10px;"}
:::
- Another example: Imagine you are studying mutual fund performance, and you only look at funds that are still in operation. You see that they beat the S&P 500 by 2% per year
- What's the problem here?
:::
:::{.column width=50%}
:::{style="text-align: center;"}
![](figures/aircraft.png){width=80%}
:::
:::
:::
:::

## Post-treatment bias

:::{style="margin-top: 30px; font-size: 20px;"}
:::{.columns}
:::{.column width=50%}
- [Post-treatment bias]{.alert} (or [overcontrol]{.alert}) occurs when [we control for variables that are affected by the treatment]{.alert}
- Imagine a research study evaluating a job training programme for unemployed individuals
- The treatment is the job training programme and the outcome is finding employment after program completion
- Imagine also that people who take the programme submit more job applications after training
- This is a post-treatment variable, as it is affected by the treatment
- If we control for the number of job applications in our analysis, we (usually) underestimate the programme's effectiveness, because we are removing one of the mechanisms through which the treatment works
:::

:::{.column width=50%}
:::{style="text-align: center;"}
![](figures/post-treatment-bias.png){width=80%}

Source: [Elwert and Winship (2014)](https://www.annualreviews.org/content/journals/10.1146/annurev-soc-071913-043455)
:::

:::{style="text-align: left;"}
- This bias, however, can go in either direction, it depends on the context
- Is there a statistical fix for it? [Not really]{.alert}!
- The best way to avoid post-treatment bias is to [think carefully about the variables you include in your analysis]{.alert}
:::
:::
:::
:::

## Examples of post-treatment bias

:::{style="margin-top: 30px; font-size: 22px;"}
:::{.columns}
:::{.column width=50%}
### Avoidable post-treatment bias

- Causal effect of party ID on the vote 
  - Do control for race 
  - Do not control for voting intentions five minutes before voting
- Causal effect of race on salary in a firm 
  - Do control for qualifications 
  - Don’t control for position in the firm
- Causal effect of medicine on health 
  - Do control for health prior to the treatment decision 
  - Do not control for side effects or other medicines taken later
:::

:::{.column width=50%}
### Unavoidable post-treatment bias

- Causal effect of democratisation on civil war; do we control for GDP?  
  - Yes, since GDP → democratisation we must control to avoid omitted variable bias
  - No, since democratisation → GDP, we would have post-treatment bias

- Causal effect of legislative effectiveness on state failure; do we control for trade openness?
  - Yes, since trade openness → legislative effectiveness, we must control to avoid omitted variable bias 
  - No, since legislative effectiveness → trade openness, we would have post-treatment bias
:::
:::
:::

## Now it's your turn! 🤓

:::{style="margin-top: 30px; font-size: 26px;"}
- I will list some factors and you tell me if they are likely to be sources of selection bias (and why)
  - Telephone random sampling
  - Ascertainment: when the investigator responsible for assessing the outcome of interest is also aware of the group assignment
  - [WEIRD samples](https://www.apa.org/monitor/2010/05/weird): Western, Educated, Industrialised, Rich, and Democratic
  - Referral filter: when the study population is recruited through referrals, often from health care providers or NGOs
  - When you ask socially sensitive questions in a survey
:::

# Measuring variability 📏 {background-color="#2d4563"}

## Measuring variability

:::{style="margin-top: 30px; font-size: 18px;"}
:::{.columns}
:::{.column width=50%}
### Variance and standard deviation

- Apart from averages, we are also interested in [variability]{.alert}
- We usually look at [average squared deviations from the mean]{.alert} (variance) 
- The sample variance of $Y_i$ in a sample of size $n$ is defined as:
  - $S(Y_i)^2 = \frac{1}{n-1} \sum_{i=1}^{n} (Y_i - \bar{Y})^2$
- The [population variance]{.alert}, which is a fixed parameter, is defined as:
  - $Var(Y_i) = E[(Y_i - E[Y_i])^2]$ 
  - Here we just replaced averages with expectations
- Since the variance can be quite large, we often look at the [standard deviation]{.alert} (square root of the variance), written as $\sigma(Y_i)$
- These measures are [descriptive]{.alert}, as they summarise the variability in your dataset
:::

:::{.column width=50%}
### Standard error

- The standard deviation of an inferential statistic is called its [standard error]{.alert}
- Standard error measures [how precise your sample mean is as an estimate of the true population mean]{.alert} (which we usually don't know) 
- The standard error of the sample mean can be written as:
  - $SE(\bar{Y}) = \frac{\sigma(Y_i)}{\sqrt{n}}$
- But since we usually don't have the population standard deviation, we use the sample standard deviation instead
  - $SE(\bar{Y}) = \frac{S(Y_i)}{\sqrt{n}}$
- [The standard error is not the same as the standard deviation]{.alert}
- The standard error is a measure of the [precision]{.alert} of an estimate, while the standard deviation is a measure of the [variability of the data]{.alert}
- Confused? 🤔 Don't worry, let's see their differences in further detail 😉
:::
:::
:::

## Comparison between standard deviation and standard error

:::{style="margin-top: 30px; font-size: 22px;"}
:::{.columns}
:::{.column width=50%}
- A high SD indicates more spread in the data points. It [does not inherently change with sample size]{.alert}
- [The standard error decreases as the sample size increases]{.alert}, indicating that [larger samples yield more reliable estimates of the mean]{.alert}
  - That's the central limit theorem at work! 🤓
- Confidence intervals in statistial models are often constructed using the SE, while descriptive statistics often report the SD
- Does it sound clearer now? 🙂
:::

:::{.column width=50%}
- Suppose you measure the IQ of 50 students: 
  - $\bar{IQ} = 100$ 
  - $SD = 15$ 
  - $SE = \frac{15}{\sqrt{50}} \approx 2.12$
- $SD = 15$: Individual IQs vary by 15 points from the mean (e.g., most are between 85 and 115).
- $SE = 2.12$: If you repeated the study, the average IQ of your samples would typically vary by 2 points from 100
:::
:::
:::

## Standard error of a difference in means

:::{style="margin-top: 30px; font-size: 19px;"}
- We are usually interested not in one mean, but in the difference between two means
- Let's assume we have a treatment group mean of $\bar{Y}_{1}$ and a control group mean of $\bar{Y}_{0}$, with sample sizes $n_1$ and $n_0$, respectively
- The total sample size is $n = n_1 + n_0$
- Using the fact that the variance of a difference between two statistically independent variables is the sum of their variances, we have:
  - $Var(\bar{Y}_{1} - \bar{Y}_{0}) = Var(\bar{Y}_{1}) + Var(\bar{Y}_{0})$
  - $Var(\bar{Y}_{1} - \bar{Y}_{0}) = \frac{\sigma(Y_{1})^2}{n_1} + \frac{\sigma(Y_{0})^2}{n_0}$
- The standard error of the difference in means is then:
  - $SE(\bar{Y}_{1} - \bar{Y}_{0}) = \sqrt{\frac{\sigma(Y_{1})^2}{n_1} + \frac{\sigma(Y_{0})^2}{n_0}}$
- Again, since we don't usually have the population standard deviation, we use the sample standard deviation instead
  - $SE(\bar{Y}_{1} - \bar{Y}_{0}) = \sqrt{\frac{S(Y_{1})^2}{n_1} + \frac{S(Y_{0})^2}{n_0}}$
- This is the standard error of the difference in means, which is used to calculate confidence intervals and hypothesis tests: $t = \frac{\text{Difference}}{\text{SE}_{\text{diff}}}$ and $CI = \text{Difference} \pm t \times \text{SE}_{\text{diff}}$
- Do you have to worry about this? Not really, but it's good to know what's going on under the hood 😉
- [Your software will do the heavy lifting for you]{.alert} 🤖
:::

# Regression analysis of experiments 📊 {background-color="#2d4563"}

## Regression analysis of experiments

:::{style="margin-top: 30px; font-size: 24px;"}
- It is super easy to estimate the average treatment effect using [regression analysis]{.alert}
- Although some areas (e.g. medicine, psychology) often use $t$-tests for [difference-in-means]{.alert}, regression is more flexible and gives you exactly the same results
- The idea is to [regress the outcome variable on the treatment variable]{.alert} and some covariates if necessary
- But always report the ATE without covariates first, as it is the most interpretable
- Let's see an example using `R`! 🤓
- We will the `fabricatr`, `estimatr`, and `randomizr` packages, which are part of the `DeclareDesign` suite
- More information about the packages are available [here](https://declaredesign.org/r/fabricatr/index.html), [here](https://declaredesign.org/r/randomizr/index.html), and [here](https://declaredesign.org/r/estimatr/index.html)
- We will start slowly, don't worry, and build up from there in the next lectures 😉
:::

## Simulating data in `R`

:::{style="margin-top: 30px; font-size: 20px;"}
:::{.columns}
:::{.column width=40%}
- Imagine that we want to estimate the effect of a job training programme on the number of interviews that unemployed individuals get, similar to the example we discussed earlier
- The treatment variable is `treat`, which takes the value 1 if the individual received the training and 0 otherwise
- We will assign 1000 individuals to the treatment and control groups, with an equal probability of being assigned to each group (simple random assignment)
- The outcome variable is `interviews`, which is the number of interviews the individual got after the training
- The treatment effect is 5, that is, individuals who received the training got 5 more interviews than those who did not
:::

:::{.column width=60%}
```{r}
#| echo: true
#| eval: true

# Load the required packages
library(fabricatr)
library(estimatr)
library(randomizr)

# Set the seed for reproducibility
set.seed(385)

# Simulate the data
data <- fabricate( # fabricatr
  N = 1000, # base
  treat = complete_ra(N, m = 500), # randomizr
  interviews = round(rnorm(N, mean = 10, sd = 2) + 5 * treat, digits = 0) # base
)

# Check the first few rows of the data
head(data)
```
:::
:::
:::

## Step-by-step: using `fabricatr` to simulate data

:::{style="margin-top: 30px; font-size: 20px;"}
- We used the `fabricate()` function from the `fabricatr` package to simulate the data
- The package is very flexible and was built for all types of data simulations, mainly those used by social scientists (e.g. experiments, surveys)
  - We can draw from Likert scales, create binary variables, ordinal variables, categorical variables, etc
  - More about how to create different types of variables [here](https://declaredesign.org/r/fabricatr/articles/common_social.html)
- The `fabricate()` function takes a series of arguments, which are the variables we want to simulate
- It uses base `R` functions too, which makes it easy to use
- For instance, `N = 1000` is the number of individuals in the sample and we created it using just base `R`
- To simulate the `interiews` variable, we used the `rnorm()` function to draw from a normal distribution with mean 10 and standard deviation 2
- We sum 5 because it is the treatment effect
- As we cannot have a fraction of an interview, we used the `round()` function to round the number of interviews to the nearest integer (`digits = 0`)
- So far so good? 🤓
:::

## Randomising treatment assignment

:::{style="margin-top: 30px; font-size: 20px;"}
- We used the `complete_ra()` function from the `randomizr` package to randomise the treatment assignment
- The function has a few arguments, such as
  - `N`: the number of individuals in the sample (required)
  - `m`: the number of individuals assigned to the treatment group (optional)
  - `m_each`: the number of individuals assigned to each treatment group (optional)
- We could have used `simple_ra()` for simple random assignment, which is a simple coin flip. The arguments are:
  - `N`: the number of individuals in the sample (required)
  - `prob`: the probability of being assigned to the treatment group (optional)
  - `prob_each`: the probability of being assigned to each treatment group (optional)
- We could also use `block_ra()` for block random assignment, which is when we divide the sample into blocks and then randomise within each block 
- `cluster_ra()` is used for cluster random assignment, which is when we randomise at the cluster level (e.g. schools, hospitals)
- Finally, `block_and_cluster_ra()` is used for block and cluster random assignment, which is a combination of the two
- We will discuss these functions in further detail in the next lectures 😉
:::

## Estimating the treatment effect

:::{style="margin-top: 30px; font-size: 20px;"}
:::{.columns}
:::{.column width=35%}
- Now that we have the data, we can estimate the treatment effect using the `lm_robust()` function from the `estimatr` package
- The function is similar to the `lm()` function, but it uses robust standard errors, which are more appropriate for experiments
- Why? Because [robust standard errors correct for heteroskedasticity and clustering](https://declaredesign.org/r/estimatr/articles/mathematical-notes.html#lm_robust-notes)
- This is a free lunch: if your data is not heteroskedastic or clustered, the robust standard errors will be the same as the regular standard errors
- The function takes a formula as an argument, which is the outcome variable regressed on the treatment variable
:::

:::{.column width=65%}
```{r}
#| echo: true
#| eval: true
model <- lm_robust(interviews ~ treat, data = data)
summary(model)
```
:::
:::
:::

## Difference in means

:::{style="margin-top: 30px; font-size: 24px;"}
- For those who would like to estimate the difference in means using the `t.test()` function, we've got you covered!  😉
- `estimatr` also has a function that does that, called `difference_in_means()` (what a surprise! 😂)
- It is actually more flexible than the `t.test()` function, as it can handle unit-randomised, cluster-randomised, block-randomised, matched-pair randomised, and matched-pair clustered designs. More [here](https://declaredesign.org/r/estimatr/articles/getting-started.html#difference_in_means)
- Here is how to use it:

```{r}
#| echo: true
#| eval: true

difference_in_means(interviews ~ treat, data = data)
```

- As you can see, the results are exactly the same as the regression model
- Just the output looks a little different, but the interpretation is the same
- Use whatever you feel more comfortable with! 😊
:::

## Interpreting the results

:::{style="margin-top: 30px; font-size: 19px;"}
:::{.columns}
:::{.column width=35%}
- The output of the `summary()` function gives us the [coefficient estimate]{.alert} of the treatment variable
- The intercept is `10.18`, which is the average number of interviews for the control group
- The coefficient of the treatment variable is `4.91`, which is the average treatment effect
- The standard error of the treatment effect is `0.13`, which is the standard error of the difference in means
- Our results are statistically significant at the 5% level, as the $p$-value is less than 0.05
:::

:::{.column width=65%}
```{r}
#| echo: true
#| eval: true
model <- lm_robust(interviews ~ treat, data = data)
summary(model)
```
- The `R^2` and its adjusted version are measures of the goodness of fit of the model
  - They are the proportion of the variance in the outcome variable explained by the model
- The `F` statistic is a test of the overall significance of the model
  - The null hypothesis is that all coefficients are zero
  - But as we see, the treatment variable is significant
:::
:::
:::

## Including pre-treatment covariates in the model

:::{style="margin-top: 30px; font-size: 20px;"}
:::{.columns}
:::{.column width=50%}
- We can also include covariates in the model to [increase precision]{.alert}
- In contrast with the treatment, the covariates have [no causal interpretation]{.alert}, that is, they are not affected by the treatment
- Even with randomisation, there can be [small imbalances]{.alert} in covariates between treatment and control groups, especially in small samples
- Including these covariates in the regression helps to adjust for any residual imbalances that may exist due to random chance, making the analysis more accurate
- Including covariates can also help bolster the credibility of the results by demonstrating that the treatment effect persists even after controlling for relevant variables
- But remember to include [only pre-treatment covariates]{.alert} in the model, as post-treatment covariates can introduce bias
:::

:::{.column width=50%}
- Let's simulate a new dataset with two pre-treatment covariates: `age` and `education`

```{r}
#| echo: true
#| eval: true
set.seed(385)

data2 <- fabricate(
  N = 1000,
  treat = complete_ra(N, m = 500),
  age = round(rnorm(N, mean = 30, sd = 5)),
  education = round(rnorm(N, mean = 12, sd = 2)),
  interviews = round(rnorm(N, mean = 10, sd = 2) + 5 * treat)
)

head(data2)
```
:::
:::
:::

## Estimating the second model

:::{style="margin-top: 30px; font-size: 23px;"}
- We can add covariates to experimental models to increase precision
- The same way we do in any regression:
  - $Y_i = \alpha + \beta \text{T}_i + \gamma \text{X}_i + \epsilon_i$
- [Freedman (2008)](https://doi.org/10.1214/07-AOAS143) demonstrated that pre-treatment covariate adjustment may bias estimates of average treatment effects, mainly in small samples
- [Lin (2013)](https://doi.org/10.1214/12-AOAS583) proposed that if covariate correlations differ between treatment and control groups, centering and interacting covariates with the treatment variable will balance the groups
- This adjusts for covariates separately in each group, which is equivalent to including treatment-by-covariate interactions
  - $Y_i = \alpha + \beta \text{T}_i + \gamma \text{W}_i + \delta \text{T}_i \times \text{W}_i + \epsilon_i$
  - Where $W_i = \text{X}_i - \bar{\text{X}}$ and $\bar{\text{X}}$ is the mean of $\text{X}_i$
  - For ease of interpretation, we can centre the covariates to have a mean of zero
- Anyway, worry not! 😅 Our friends at [DeclareDesign](https://declaredesign.org/r/estimatr/articles/getting-started.html#lm_lin) have done all the work for us and created a function called `lm_lin()` that does everything automatically!
:::

## Estimating the second model

:::{style="margin-top: 30px; font-size: 20px;"}

:::{.columns}
:::{.column width=65%}
```{r}
#| echo: true
#| eval: true
model2 <- lm_lin(interviews ~ treat,
                covariates = ~ age + education,
                data = data2)
summary(model2)
```
:::

:::{.column width=35%}
- The `lm_lin()` function is similar to the `lm_robust()` function, but it includes the `covariates` argument
- We pass a simple `R` formula to it
- As you can see, the results are pretty similar to the previous model
- The treatment effect is slightly higher, and the standard error is slightly lower
- The `_c` part of the variable names indicates that the variables are centred, as recommended by Lin (2013).
- Lin's method addresses covariate imbalance by adjusting for covariates separately within each treatment group, which is equivalent to including all treatment-by-covariate interactions.
:::
:::
:::

## Sub-group analysis

:::{style="margin-top: 30px; font-size: 20px;"}
:::{.columns}
:::{.column width=35%}
- We can also estimate the treatment effect for sub-groups of the population
- This is useful when we suspect that the treatment effect may vary across known dimensions
- For instance, we can estimate the treatment effect for people with high and low levels of education
- We can do this by [including an interaction term between the treatment variable and the covariate of interest]{.alert}
- We will discuss this in further detail in the next lectures, but here is how to do it
:::

:::{.column width=65%}
```{r}
#| echo: true
#| eval: true
# Create a binary subgroup (e.g., "high" vs. "low" education)
data2$high_edu <- ifelse(data2$education > median(data2$education), 1, 0)

# Fit an interaction model with covariates
model_interaction <- lm_robust(
  interviews ~ treat * high_edu + age + education,
  data = data2)

# Summarize results
summary(model_interaction)
```
:::
:::
:::

## Sub-group analysis: interpreting the results

:::{style="margin-top: 30px; font-size: 19px;"}
:::{.columns}
:::{.column width=35%}
- The output of the `summary()` function gives us the [coefficient estimate]{.alert} of the treatment variable for the high education group 
  - It is in the last line, `treat:high_edu`
- [Interpretation]{.alert}: The treatment effect for the high-education subgroup (`high_edu = 1`) is 0.097 larger than for the low-education subgroup 
- Total treatment effect for `high_edu = 1: 5.101 + 0.097 = 5.198` 
- [Significance]{.alert}: The interaction is not significant (CI: -0.395 to 0.590), meaning there’s no evidence the treatment effect differs between education subgroups
- This is expected, as we simulated the data to have the same treatment effect for all individuals
- But it is good to know how to do it, right? 😉
:::

:::{.column width=65%}
```{r}
#| echo: true
#| eval: true
# Create a binary subgroup (e.g., "high" vs. "low" education)
data2$high_edu <- ifelse(data2$education > median(data2$education), 1, 0)

# Fit an interaction model with covariates
model_interaction <- lm_robust(
  interviews ~ treat * high_edu + age + education,
  data = data2)

# Summarize results
summary(model_interaction)
```
:::
:::
:::

# Cool, isn't it? 😎 {background-color="#2d4563"}

# Questions? {background-color="#2d4563"}

## Summary

:::{style="margin-top: 30px; font-size: 20px;"}
:::{.columns}
:::{.column width=60%}
- We discussed the potential outcomes framework in further detail, focusing on issues of selection bias
- We talked about different types of selection bias, such as inappropriate controls, loss to follow-up, attrition bias, survivorship bias, and counfounders
- We also discussed how to measure variability using the standard deviation and standard error
- Then, we talked about regression analysis of experiments, including how to simulate data, randomise treatment assignment, estimate the treatment effect, and include pre-treatment covariates in the model
- We also introduced the `fabricatr`, `estimatr`, and `randomizr` packages, which are part of the `DeclareDesign` suite
- Finally, we discussed sub-group analysis and how to estimate the treatment effect for different sub-groups of the population
- I hope you enjoyed the lecture! 🤓
- Next time, we will discuss more about hypothesis testing and confidence intervals, so stay tuned! 😉
:::

:::{.column width=40%}
:::{style="text-align: center;"}
![](figures/meme.jpg){width=100%}
:::
:::
:::
:::

# Thank you and see you soon! 😃 {background-color="#2d4563"}
