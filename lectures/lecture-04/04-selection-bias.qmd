---
title: QTM 385 - Experimental Methods
subtitle: Lecture 04 - Selection Bias
author:
  - name: Danilo Freire
    email: danilo.freire@emory.edu
    affiliations: Emory University
format:
  clean-revealjs:
    self-contained: true
    code-overflow: wrap
    footer: "[Selection Bias](https://raw.githack.com/danilofreire/qtm385/main/lectures/lecture-04/04-selection-bias.html)"
transition: slide
transition-speed: default
scrollable: true
engine: knitr
revealjs-plugins:
  - multimodal
editor:
  render-on-save: true
---

# Hello, everyone! üëã <br> Hope you're all doing well! üòâ {background-color="#2d4563"}

# Brief recap üìö {background-color="#2d4563"}

## Last time, we saw that...

:::{style="margin-top: 30px; font-size: 21px;"}
:::{.columns}
:::{.column width=50%}
- [Potential outcomes framework]{.alert} help understand treatment effects
- [Fundamental problem of causal inference]{.alert}: we only see one potential outcome
- [Treatment effect]{.alert} is the difference between potential outcomes ($\tau_{i} = Y_{1,i} - Y_{0,i}$)
- [Average causal effect]{.alert} is the goal, but simple means comparison is biased
- [Selection bias]{.alert}: treated/untreated groups differ even without treatment
- [Bias]{.alert} does not vanish in large samples
- [Mathematical expectation]{.alert} $E[Y_i]$ is the population average
- [Law of large numbers]{.alert}: sample average converges to population average
- [Conditional expectation]{.alert} $E[Y_i|X_i]$ is average outcome given $X_i$
:::

:::{.column width=50%}
- [Experimental ideal]{.alert}: random assignment to treatment
- [Random assignment]{.alert} creates similar groups on average
- [SUTVA]{.alert} (Stable Unit Treatment Value Assumption) 
- [Violations of SUTVA]{.alert}: spillovers, interference, contamination
- [Partial equilibrium]{.alert}: treatment effects may not generalise
- [External validity]{.alert}: generalising results to other settings
- [Heterogeneous treatment effects]{.alert}: effects vary across people
- [Compliance problem]{.alert}: people may not follow random assignment
- [Intent-to-Treat (ITT)]{.alert} and [Treatment-on-the-Treated (TOT)]{.alert} analyses address compliance
:::
:::
:::

## Today, we will discuss...

:::{style="margin-top: 50px; font-size: 28px;"}
:::{.columns}
:::{.column width=50%}
- Potential outcomes in further detail
- Different types of selection bias, e.g.:
  - Inappropriate controls
  - Loss to follow-up
  - Volunteer bias
  - Non-response bias
- How to address selection bias
- How to use `R` to estimate regression models
- [But first...]{.alert}
:::

:::{.column width=50%}
:::{style="text-align: center; margin-top: -40px;"}
![](figures/selection-bias.png){width=80%}

Source: [xkcd](https://xkcd.com/2618/) (who else? üòÑ)
:::
:::
:::
:::

## Funny correlation of the day üòÇ

:::{style="margin-top: 30px; font-size: 21px; text-align: center;"}
![](figures/correlation.png){width=80%}
:::

# Let's get started! üöÄ {background-color="#2d4563"}

## Potential outcomes revisited

:::{style="margin-top: 30px; font-size: 25px;"}
- Remember from last time: we have two potential outcomes for each unit
- [We only observe one of them]{.alert}: $Y_i = Y_{1,i} \cdot D_i + Y_{0,i} \cdot (1 - D_i)$
- So we need to estimate the [average treatment effect (ATE)]{.alert}:
  - $E[Y_{1,i} - Y_{0,i}] = E[Y_{1,i}] - E[Y_{0,i}]$
- The problem is, [not all comparisons are valid]{.alert}
- I mean, they can be, but only under the [heroic assumption]{.alert} that the groups are similar on average before any adjustment
- Otherwise, we will have [selection bias]{.alert}
- Let's see how this works in practice! ü§ì
:::

## Khuzdar and Maria

:::{style="margin-top: 30px; font-size: 21px;"}
:::{.columns}
:::{.column width=50%}
- [Selection bias]{.alert} occurs when the groups are different even without treatment
- This can happen for many reasons, but first let's see the (simple) maths behind it
- Using an example from Angrist and Pischke (2021), let's say we have a student called [Khuzdar]{.alert} from Kazakhstan, who is considering studying in the US and is worried about the cold weather
- Should he get health insurance? ü§î
- Let's imagine that, without insurance, Khuzdar has a potential outcome of $Y_{0,i} = 3$ and, with insurance, $Y_{1,i} = 4$. So the treatment effect is $\tau_i = 1$, that is, he gains 1 "health point" by getting insurance
:::

:::{.column width=50%}
- Now, let's imagine that Khuzdar has a Chilean colleague called [Maria Moreno]{.alert}, who is also considering studying in the US
- But since she comes from chilly Santiago, she is not worried about the cold weather
- So, without insurance, Maria has a potential outcome of $Y_{0,i} = 5$ and, with insurance, $Y_{1,i} = 5$. So the treatment effect is $\tau_i = 0$, that is, she gains no "health points" by getting insurance

:::{style="text-align: center;"}
![](figures/khuzdar-maria.png){width=80%}
:::
:::
:::
:::

## Khuzdar and Maria

:::{style="margin-top: 30px; font-size: 21px;"}
- In fact, the comparison between frail Khuzdar and hearty Maria tells us little about the causal effects of their choices!
- Why is that? Because [they were different to begin with]{.alert}
- Let's do a little mathematical trick here: we will add and subtract $Y_{0, Khuzdar}$ from the treatment effect (they cancel each other out, right?)
- So we have the following:

:::{style="text-align: center;"}
![](figures/selection-bias-math.png){width=80%}
:::
:::

## Difference in means = average treatment effect + selection bias

:::{style="margin-top: 30px; font-size: 20px;"}
:::{.columns}
:::{.column width=50%}
- The second term is the [selection bias]{.alert}
- The same is true for averages: [the difference in means is the average treatment effect plus the selection bias]{.alert}
- Imagine that we have a dummy variable $D_i$ for treatment, which takes the value 1 if the unit is treated (in our case, insured) and 0 otherwise
- Thus:

:::{style="text-align: center;"}
![](figures/selection-bias-math2.png){width=80%}
:::
:::

:::{.column width=50%}
- So far, so good, right? 
- If we assume that [the treatment has a constant effect]{.alert} (i.e. the treatment effect is the same for everyone), we can rewrite the equation as:

:::{style="text-align: center;"}
![](figures/selection-bias3.png){width=30%}
:::

- Where $k$ is both the individual and average causal effect of insurance on health
- Using the constant-effects model to substitute for $Avg_n[Y_{1i}|D_i = 1]$, we have

:::{style="text-align: center;"}
![](figures/selection-bias4.png){width=80%}
:::
:::
:::
:::

## How to check for selection bias?
### Balance tests

:::{style="margin-top: 30px; font-size: 18px;"}
:::{.columns}
:::{.column width=40%}
- We use [balance tests]{.alert} or ([randomisation checks]{.alert}) to check if the groups are similar before treatment
- The idea is to compare the means of the covariates for the treated and untreated groups
- If the means are similar, it provides evidence that nothing systematic is driving the treatment effect
- We are never 100% sure, but [we trust the random assignment]{.alert}
- Small differences in means are acceptable, as long as they are not systematic
- Why? Because some variation can happen only by chance
- [100% personal opinion]{.alert}: I think they aren't very useful üòÇ
- [Mutz et al (2018)](https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1322143) make a good case for that
:::

:::{.column width=60%}
:::{style="text-align: center;"}
![](figures/balance-test.png){width=80%}

Angrist and Pischke (2021), pp. 20 (selected parts)
:::
:::
:::
:::

# Questions? ü§î {background-color="#2d4563"}

# Different types of selection bias üéØ {background-color="#2d4563"}

## Causal diagrams and associations

:::{style="margin-top: 30px; font-size: 23px;"}
:::{.columns}
:::{.column width=50%}
- [Causal diagrams]{.alert} are a great way to visualise the relationships between variables
- They have been introduced by [Judea Pearl](https://en.wikipedia.org/wiki/Judea_Pearl) and are widely used in [causal inference]{.alert}
- They help us understand the [associations]{.alert} between variables and identify potential sources of bias
- And they are super easy to draw! ü§ì
- For instance, let's illustrate a classic example of selection bias with a causal diagram (borrowed from [Hern√°n et al., 2019](https://doi.org/10.1097/01.ede.0000135174.63482.43))
:::

:::{.column width=50%}
:::{style="text-align: center;"}
![](figures/causal-diagram.png){width=80%}
:::

:::{style="text-align: left;"}
- Figure 1 shows the common cause $L$ (smoking) results in $E$ (carrying matches) and $D$ (lung cancer) being associated
- Figure 2 shows how randomisation breaks the association between $L$ and $D$, so the treatment effect is unbiased
- In this case, if you know what $L$ is, you can also [control for it]{.alert} in your analysis
:::
:::
:::
:::

## Some sources of selection bias
### Selective survival and losses to follow-up

:::{style="margin-top: 30px; font-size: 18px;"}
:::{.columns}
:::{.column width=50%}
- One problem that we sometimes have with experiments is [selective survival]{.alert}
- That is, when individuals leave the study before the end of follow-up
- Why is that an issue?
- Because the [treatment effect may be different for those who leave]{.alert}
- Imagine a follow-up study of the effect of antiretroviral therapy ($E$) on AIDS ($D$) risk among HIV-infected patients 
- $L$ is the presence of other symptoms of HIV infection 
- The greater the true level of immunosuppression ($U$), the greater the risk of AIDS. $U$ is unmeasured
- If a patient drops out from the study, his AIDS status cannot be assessed and we say that he is censored ($C=1$) 
- Patients with greater values of $U$ are more likely to be lost to follow up
- The [causal diagram]{.alert} shows the relationship between the variables
:::

:::{.column width=50%}
:::{style="text-align: center;"}
![](figures/selective-survival.png){width=80%}
:::

:::{style="text-align: left;"}
- How can we correct for this bias?
- One way is to use [stratified analysis]{.alert} or [inverse probability weighting]{.alert}
- We will discuss these methods in further detail in the next lectures
- But stratification is quite simple:
  - Assuming that $L$ is measured, we can estimate the treatment effect within each stratum of $L$
  - Then we can average the stratum-specific estimates to get the overall treatment effect
:::
:::
:::
:::

## Survivorship bias

:::{style="margin-top: 30px; font-size: 26px;"}
- In my humble view, this is one of the coolest examples of selection bias
- [Survivorship bias]{.alert} is the logical error of concentrating on the people or things that made it past some selection process and overlooking those that did not
- It is a type of [selection bias]{.alert} that can lead to [false conclusions]{.alert} in several different ways
- For instance, imagine that you are studying the [effect of education on income]{.alert}
- If you only look at people who have completed college, you are ignoring those who dropped out
- This can lead to an [overestimation]{.alert} of the effect of education on income
- Why? Because those who dropped out may have done so because they were not good students, which can affect their income
- And there are many other examples of survivorship bias, such as in [historical studies]{.alert} and [finance]{.alert}
- Have a look at this one!
:::

## Survivorship bias in World War II aircraft

:::{style="margin-top: 30px; font-size: 26px;"}
:::{.columns}
:::{.column width=50%}
- During World War II, the US military wanted to [improve the armour on its aircraft]{.alert}
- They analysed the [bullet holes]{.alert} on returning aircraft and decided to add more armour to the areas with the most bullet holes
- Considering the picture you see here, where would you add the armour?
- More [here](https://en.wikipedia.org/wiki/Survivorship_bias#Military)
:::

:::{.column width=50%}
:::{style="text-align: center;"}
![](figures/aircraft.png){width=80%}
:::
:::
:::
:::

## Post-treatment bias

:::{style="margin-top: 30px; font-size: 20px;"}
:::{.columns}
:::{.column width=50%}
- [Post-treatment bias]{.alert} (or [overcontrol]{.alert}) occurs when [we control for variables that are affected by the treatment]{.alert}
- This can block the pathway through which the treatment affects the outcome, leading to an underestimation of the treatment effect
- Imagine a research study evaluating a job training programme for unemployed individuals
- The treatment is the job training programme and the outcome is finding employment after program completion
- Imagine also that people who take the programme submit more job applications after training
- This is a post-treatment variable, as it is affected by the treatment
- If we control for the number of job applications in our analysis, we underestimate the programme's effectiveness, because we are removing one of the mechanisms through which the treatment works
:::

:::{.column width=50%}
:::{style="text-align: center;"}
![](figures/post-treatment-bias.png){width=80%}

Source: [Elwert and Winship (2014)](https://www.annualreviews.org/content/journals/10.1146/annurev-soc-071913-043455)
:::

:::{style="text-align: left;"}
- This bias can go in either direction, it depends on the context
- Is there a statistical fix for it? [Not really]{.alert}!
- The best way to avoid post-treatment bias is to [think carefully about the variables you include in your analysis]{.alert}
:::
:::
:::
:::

## Now it's your turn! ü§ì

:::{style="margin-top: 30px; font-size: 26px;"}
- I will list some factors and you tell me if they are likely to be sources of selection bias (and why)
  - Telephone random sampling
  - Ascertainment: when the investigator responsible for assessing the outcome of interest is also aware of the group assignment
  - [WEIRD samples](https://www.apa.org/monitor/2010/05/weird): Western, Educated, Industrialised, Rich, and Democratic
  - Referral filter: when the study population is recruited through referrals, often from health care providers or NGOs
  - When you ask socially sensitive questions in a survey
:::

# Measuring variability üìè {background-color="#2d4563"}

## Measuring variability

:::{style="margin-top: 30px; font-size: 18px;"}
:::{.columns}
:::{.column width=50%}
### Variance and standard deviation

- Apart from averages, we are also interested in [variability]{.alert}
- We usually look at [average squared deviations from the mean]{.alert} (variance) 
- The sample variance of $Y_i$ in a sample of size $n$ is defined as:
  - $S(Y_i)^2 = \frac{1}{n-1} \sum_{i=1}^{n} (Y_i - \bar{Y})^2$
- The [population variance]{.alert}, which is a fixed parameter, is defined as:
  - $Var(Y_i) = E[(Y_i - E[Y_i])^2]$ 
  - Here we just replaced averages with expectations
- Since the variance can be quite large, we often look at the [standard deviation]{.alert} (square root of the variance), written as $\sigma(Y_i)$
:::

:::{.column width=50%}
### Standard error

- The standard deviation of a statistic like the sample average is called its [standard error]{.alert}
- The standard error measures [the dispersion of sample means around the true population mean]{.alert}
- The standard error of the sample mean can be written as:
  - $SE(\bar{Y}) = \frac{\sigma(Y_i)}{\sqrt{n}}$
- But since we usually don't have the population standard deviation, we use the sample standard deviation instead
  - $SE(\bar{Y}) = \frac{S(Y_i)}{\sqrt{n}}$
- There are many ways to calculate the standard error, but the idea is the same: [it summarises the variability in an estimate due to random sampling]{.alert}
- Although similar, the standard error is not the same as the standard deviation
- The standard error is a measure of the [precision]{.alert} of an estimate, while the standard deviation is a measure of the variability of the data
- Confused? ü§î Don't worry, let's see their differences in further detail üòâ
:::
:::
:::

## Comparison between standard deviation and standard error

:::{style="margin-top: 30px; font-size: 22px;"}
- [Standard Deviation (SD)]{.alert}: Measures the dispersion of individual data points in a dataset around the mean
  - It provides information about how spread out the data values are
- [Standard Error (SE)]{.alert}: Measures the dispersion of sample means
  - It provides an estimate of how much the sample mean is expected to vary from the actual population mean
- A high SD indicates more spread in the data points. It [does not inherently change with sample size]{.alert}
- [The standard error decreases as the sample size increases]{.alert}, indicating that [larger samples yield more reliable estimates of the mean]{.alert}
  - That's the central limit theorem at work! ü§ì
- Confidence intervals in statistial models are often constructed using the SE, while descriptive statistics often report the SD
- Does it sound clearer now? üôÇ
:::

## Standard error of a difference in means

:::{style="margin-top: 30px; font-size: 19px;"}
- We are usually interested not in one mean, but in the difference between two means
- Let's assume we have a treatment group mean of $\bar{Y}_{1}$ and a control group mean of $\bar{Y}_{0}$, with sample sizes $n_1$ and $n_0$, respectively
- The total sample size is $n = n_1 + n_0$
- Using the fact that the variance of a difference between two statistically independent variables is the sum of their variances, we have:
  - $Var(\bar{Y}_{1} - \bar{Y}_{0}) = Var(\bar{Y}_{1}) + Var(\bar{Y}_{0})$
  - $Var(\bar{Y}_{1} - \bar{Y}_{0}) = \frac{\sigma(Y_{1})^2}{n_1} + \frac{\sigma(Y_{0})^2}{n_0}$
- The standard error of the difference in means is then:
  - $SE(\bar{Y}_{1} - \bar{Y}_{0}) = \sqrt{\frac{\sigma(Y_{1})^2}{n_1} + \frac{\sigma(Y_{0})^2}{n_0}}$
- Again, since we don't usually have the population standard deviation, we use the sample standard deviation instead
  - $SE(\bar{Y}_{1} - \bar{Y}_{0}) = \sqrt{\frac{S(Y_{1})^2}{n_1} + \frac{S(Y_{0})^2}{n_0}}$
- This is the standard error of the difference in means, which is used to calculate confidence intervals and hypothesis tests üòé
- Do you have to worry about this? Not really, but it's good to know what's going on under the hood üòâ
- [Your software will do the heavy lifting for you]{.alert} ü§ñ
:::

# Regression analysis of experiments üìä {background-color="#2d4563"}

## Regression analysis of experiments

:::{style="margin-top: 30px; font-size: 24px;"}
- It is super easy to estimate the average treatment effect using [regression analysis]{.alert}
- Although some areas (e.g. medicine, psychology) often use $t$-tests for [difference-in-means]{.alert}, regression is more flexible and gives you exactly the same results
- The idea is to [regress the outcome variable on the treatment variable]{.alert} and some covariates if necessary
- But always report the ATE without covariates first, as it is the most interpretable
- Let's see an example using `R`! ü§ì
- Here we will start with the `lm()` function, which is the basic regression function in `R`
- Then, we will the `fabricatr`, `estimatr`, and `randomizr` packages, which are part of the `DeclareDesign` suite
- More information about the packages are available [here](https://declaredesign.org/r/fabricatr/index.html), [here](https://declaredesign.org/r/randomizr/index.html), and [here](https://declaredesign.org/r/estimatr/index.html)
- We will start slowly, don't worry, and build up from there in the next lectures üòâ
:::

## Simulating data in `R`

:::{style="margin-top: 30px; font-size: 20px;"}
:::{.columns}
:::{.column width=40%}
- Imagine that we want to estimate the effect of a job training programme on the number of interviews that unemployed individuals get, similar to the example we discussed earlier
- The treatment variable is `treat`, which takes the value 1 if the individual received the training and 0 otherwise
- We will assign 1000 individuals to the treatment and control groups, with an equal probability of being assigned to each group (simple random assignment)
- The outcome variable is `interviews`, which is the number of interviews the individual got after the training
- The treatment effect is 5, that is, individuals who received the training got 5 more interviews than those who did not
:::

:::{.column width=60%}
```{r}
#| echo: true
#| eval: true

# Load the required packages
library(fabricatr)
library(estimatr)
library(randomizr)

# Set the seed for reproducibility
set.seed(385)

# Simulate the data
data <- fabricate( # fabricatr
  N = 1000, # base
  treat = complete_ra(N, m = 500), # randomizr
  interviews = round(rnorm(N, mean = 10, sd = 2) + 5 * treat, digits = 0) # base
)

# Check the first few rows of the data
head(data)
```
:::
:::
:::

## Step-by-step: using `fabricatr` to simulate data

:::{style="margin-top: 30px; font-size: 20px;"}
- We used the `fabricate()` function from the `fabricatr` package to simulate the data
- The package is very flexible and was built for all types of data simulations, mainly those used by social scientists (e.g. experiments, surveys)
  - We can draw from Likert scales, create binary variables, ordinal variables, categorical variables, etc
  - More about how to create different types of variables [here](https://declaredesign.org/r/fabricatr/articles/common_social.html)
- The `fabricate()` function takes a series of arguments, which are the variables we want to simulate
- It uses base `R` functions too, which makes it easy to use
- For instance, `N = 1000` is the number of individuals in the sample and we created it using just base `R`
- To simulate the `interiews` variable, we used the `rnorm()` function to draw from a normal distribution with mean 10 and standard deviation 2
- We sum 5 because it is the treatment effect
- As we cannot have a fraction of an interview, we used the `round()` function to round the number of interviews to the nearest integer (`digits = 0`)
- So far so good? ü§ì
:::

## Randomising treatment assignment

:::{style="margin-top: 30px; font-size: 20px;"}
- We used the `complete_ra()` function from the `randomizr` package to randomise the treatment assignment
- The function has a few arguments, such as
  - `N`: the number of individuals in the sample (required)
  - `m`: the number of individuals assigned to the treatment group (optional)
  - `m_each`: the number of individuals assigned to each treatment group (optional)
- We could have used `simple_ra()` for simple random assignment, which is a simple coin flip. The arguments are:
  - `N`: the number of individuals in the sample (required)
  - `prob`: the probability of being assigned to the treatment group (optional)
  - `prob_each`: the probability of being assigned to each treatment group (optional)
- We could also use `block_ra()` for block random assignment, which is when we divide the sample into blocks and then randomise within each block (more on that in the next lectures)
- `cluster_ra()` is used for cluster random assignment, which is when we randomise at the cluster level (e.g. schools, hospitals)
- Finally, `block_and_cluster_ra()` is used for block and cluster random assignment, which is a combination of the two
- We will discuss these functions in further detail in the next lectures üòâ
:::

## Estimating the treatment effect

:::{style="margin-top: 30px; font-size: 20px;"}
:::{.columns}
:::{.column width=35%}
- Now that we have the data, we can estimate the treatment effect using the `lm_robust()` function from the `estimatr` package
- The function is similar to the `lm()` function, but it uses robust standard errors, which are more appropriate for experiments
- Why? Because [robust standard errors correct for heteroskedasticity and clustering](https://declaredesign.org/r/estimatr/articles/mathematical-notes.html#lm_robust-notes)
- This is a free lunch: if your data is not heteroskedastic or clustered, the robust standard errors will be the same as the regular standard errors
- The function takes a formula as an argument, which is the outcome variable regressed on the treatment variable
:::

:::{.column width=65%}
```{r}
#| echo: true
#| eval: true
model <- lm_robust(interviews ~ treat, data = data)
summary(model)
```
:::
:::
:::

## Difference in means

:::{style="margin-top: 30px; font-size: 24px;"}
- For those who would like to estimate the difference in means using the `t.test()` function, we've got you covered!  üòâ
- `estimatr` also has a function that does that, called `difference_in_means()` (what a surprise! üòÇ)
- It is actually far more flexible than the `t.test()` function, as it can handle unit-randomised, cluster-randomised, block-randomised, matched-pair randomised, and matched-pair clustered designs. More [here](https://declaredesign.org/r/estimatr/articles/getting-started.html#difference_in_means)
- Here is how to use it:

```{r}
#| echo: true
#| eval: true

difference_in_means(interviews ~ treat, data = data)
```

- As you can see, the results are the same as the regression model
- Just the output looks a little different, but the interpretation is the same
- Use whatever you feel more comfortable with! üòä
:::

## Interpreting the results

:::{style="margin-top: 30px; font-size: 19px;"}
:::{.columns}
:::{.column width=35%}
- The output of the `summary()` function gives us the [coefficient estimate]{.alert} of the treatment variable
- The intercept is `10.18`, which is the average number of interviews for the control group
- The coefficient of the treatment variable is `4.91`, which is the average treatment effect
- The standard error of the treatment effect is `0.13`, which is the standard error of the difference in means
- Our results are statistically significant at the 5% level, as the $p$-value is less than 0.05
:::

:::{.column width=65%}
```{r}
#| echo: true
#| eval: true
model <- lm_robust(interviews ~ treat, data = data)
summary(model)
```
- The `R^2` and its adjusted version are measures of the goodness of fit of the model
  - They are the proportion of the variance in the outcome variable explained by the model
- The `F` statistic is a test of the overall significance of the model
  - The null hypothesis is that all coefficients are zero
  - But as we see, the treatment variable is significant
:::
:::
:::

## Including pre-treatment covariates in the model

:::{style="margin-top: 30px; font-size: 20px;"}
:::{.columns}
:::{.column width=50%}
- We can also include covariates in the model to [increase precision]{.alert}
- In contrast with the treatment, the covariates have [no causal interpretation]{.alert}, that is, they are not affected by the treatment
- Even with randomisation, there can be [small imbalances]{.alert} in covariates between treatment and control groups, especially in small samples
- Including these covariates in the regression helps to adjust for any residual imbalances that may exist due to random chance, making the analysis more accurate
- Including covariates can also help bolster the credibility of the results by demonstrating that the treatment effect persists even after controlling for relevant variables
- But remember to include [only pre-treatment covariates]{.alert} in the model, as post-treatment covariates can introduce bias
:::

:::{.column width=50%}
- Let's simulate a new dataset with two pre-treatment covariates: `age` and `education`

```{r}
#| echo: true
#| eval: true
set.seed(385)

data2 <- fabricate(
  N = 1000,
  treat = complete_ra(N, m = 500),
  age = round(rnorm(N, mean = 30, sd = 5)),
  education = round(rnorm(N, mean = 12, sd = 2)),
  interviews = round(rnorm(N, mean = 10, sd = 2) + 5 * treat)
)

head(data2)
```
:::
:::
:::

## Estimating the second model

:::{style="margin-top: 30px; font-size: 26px;"}
- While it is common to add covariates to the model, [Freedman (2008)](https://doi.org/10.1214/07-AOAS143) demonstrated that pre-treatment covariate adjustment may bias estimates of average treatment effects
- Why? To put it simply, he claims that [experiments simply do not need covariates]{.alert}
- This is because OLS models assume linear effects and normal distributions, while experiments do not
- But [Lin (2013)](https://doi.org/10.1214/12-AOAS583) came up with a solution
- It involves a series of data transformations to make the covariates orthogonal to the treatment variable
- That takes a lot of pre-processing and is not very intuitive
- But worry not! üòÖ Our friends at [DeclareDesign](https://declaredesign.org/r/estimatr/articles/getting-started.html#lm_lin) have done all the work for us and created a function called `lm_lin()` that does everything automatically!
:::

## Estimating the second model

:::{style="margin-top: 30px; font-size: 20px;"}

:::{.columns}
:::{.column width=65%}
```{r}
#| echo: true
#| eval: true
model2 <- lm_lin(interviews ~ treat,
                covariates = ~ age + education,
                data = data2)
summary(model2)
```
:::

:::{.column width=35%}
- The `lm_lin()` function is similar to the `lm_robust()` function, but it includes the `covariates` argument
- We pass a simple `R` formula to it
- As you can see, the results are pretty similar to the previous model
- The treatment effect is slightly higher, and the standard error is slightly lower
- The `_c` part of the variable names indicates that the variables are centred, which is recommended by Lin (2013)
- He proposed centring all pre-treatment covariates, interacting them with the treatment variable, and regressing the outcome on the treatment, the centred pre-treatment covariates, and all of the interaction terms (üòÖ)
:::
:::
:::

## Sub-group analysis

:::{style="margin-top: 30px; font-size: 20px;"}
:::{.columns}
:::{.column width=35%}
- We can also estimate the treatment effect for sub-groups of the population
- This is useful when we suspect that the treatment effect may vary across known dimensions
- For instance, we can estimate the treatment effect for people with high and low levels of education
- We can do this by [including an interaction term between the treatment variable and the covariate of interest]{.alert}
- We will discuss this in further detail in the next lectures, but here is how to do it
:::

:::{.column width=65%}
```{r}
#| echo: true
#| eval: true
# Create a binary subgroup (e.g., "high" vs. "low" education)
data2$high_edu <- ifelse(data2$education > median(data2$education), 1, 0)

# Fit an interaction model with covariates
model_interaction <- lm_robust(
  interviews ~ treat * high_edu + age + education,
  data = data2)

# Summarize results
summary(model_interaction)
```
:::
:::
:::

## Sub-group analysis: interpreting the results

:::{style="margin-top: 30px; font-size: 19px;"}
:::{.columns}
:::{.column width=35%}
- The output of the `summary()` function gives us the [coefficient estimate]{.alert} of the treatment variable for the high education group 
  - It is in the last line, `treat:high_edu`
- [Interpretation]{.alert}: The treatment effect for the high-education subgroup (`high_edu = 1`) is 0.097 larger than for the low-education subgroup 
- Total treatment effect for `high_edu = 1: 5.101 + 0.097 = 5.198` 
- [Significance]{.alert}: The interaction is not significant (CI: -0.395 to 0.590), meaning there‚Äôs no evidence the treatment effect differs between education subgroups
- This is expected, as we simulated the data to have the same treatment effect for all individuals
- But it is good to know how to do it, right? üòâ
:::

:::{.column width=65%}
```{r}
#| echo: true
#| eval: true
# Create a binary subgroup (e.g., "high" vs. "low" education)
data2$high_edu <- ifelse(data2$education > median(data2$education), 1, 0)

# Fit an interaction model with covariates
model_interaction <- lm_robust(
  interviews ~ treat * high_edu + age + education,
  data = data2)

# Summarize results
summary(model_interaction)
```
:::
:::
:::

# Cool, isn't it? üòé {background-color="#2d4563"}

# Questions? {background-color="#2d4563"}

## Summary

:::{style="margin-top: 30px; font-size: 20px;"}
:::{.columns}
:::{.column width=60%}
- We discussed the potential outcomes framework in further detail, focusing on issues of selection bias
- We talked about different types of selection bias, such as inappropriate controls, loss to follow-up, survivorship bias, and counfounders
- We also discussed how to measure variability using the standard deviation and standard error
- Then, we talked about regression analysis of experiments, including how to simulate data, randomise treatment assignment, estimate the treatment effect, and include pre-treatment covariates in the model
- We also introduced the `fabricatr`, `estimatr`, and `randomizr` packages, which are part of the `DeclareDesign` suite
- Finally, we discussed sub-group analysis and how to estimate the treatment effect for different sub-groups of the population
- I hope you enjoyed the lecture! ü§ì
- Next time, we will discuss more about hypothesis testing and confidence intervals, so stay tuned! üòâ
:::

:::{.column width=40%}
:::{style="text-align: center;"}
![](figures/meme.jpg){width=100%}
:::
:::
:::
:::

# Thank you and see you soon! üòÉ {background-color="#2d4563"}
